{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN CREDITS DATA AND CREATE GRAPH FILES\n",
    "* Read in DISCOGS artist database to map common name aliases. \n",
    "* Read in, parse and clean Wikipedia data (ignore additional DISCOGS for now).\n",
    "* Pivot into network tables, drop small time producers and attain network layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import operator\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gzip import GzipFile\n",
    "from itertools import chain, combinations\n",
    "from collections import Counter\n",
    "from ggplot import *\n",
    "%matplotlib inline\n",
    "\n",
    "# packages for creating playlists\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "import pandas as pd\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LOAD DATAFRAMES\n",
    "all_tracks = pd.read_pickle('../02_data/all_tracks.p')\n",
    "artist_lookup = pickle.load(open('../02_data/artist_lookup.p', 'rb'))\n",
    "\n",
    "# how many additional tracks did discogs pull in?\n",
    "len(all_tracks[(all_tracks.credits.notnull()) & (all_tracks.credits!=set())])\n",
    "# not worth the effort..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load discogs artist lookups\n",
    "name_lookup = {}\n",
    "def handle_artist(_, artist):\n",
    "    if artist['data_quality'] in ['Complete And Correct', 'Correct']:\n",
    "        try:\n",
    "            variation = artist['namevariations']['name']\n",
    "            name_lookup[artist['name']] = [variation] if isinstance(variation, str) else variation\n",
    "        except:\n",
    "            name_lookup[artist['name']] = []\n",
    "    else:\n",
    "        pass\n",
    "    return True\n",
    "\n",
    "xmltodict.parse(GzipFile('../02_data/discogs_20170801_artists.xml.gz'), item_depth=2, item_callback=handle_artist)\n",
    "\n",
    "# create variations lookup.\n",
    "# if 2 variations map to the same key then nuke it\n",
    "var_lu = {}\n",
    "for name, variations in name_lookup.items():\n",
    "    if len(variations)>0:\n",
    "        for var in variations:\n",
    "            if name in var_lu:\n",
    "                var_lu[name] = None\n",
    "            else:\n",
    "                var_lu[var] = name\n",
    "\n",
    "#del name_lookup # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine most important credit info obtained from wiki\n",
    "fields = []\n",
    "for i in all_tracks.wiki:\n",
    "    try:\n",
    "        fields.extend(list(i.keys()))\n",
    "    except:\n",
    "        pass\n",
    "sorted(Counter(fields).items(), key=operator.itemgetter(1), reverse=True)[:10]\n",
    "# want to parse out: composer, producer, writer (maybe: artist, recorded, released?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_list(dirty):\n",
    "    # somg regex taken from https://github.com/Ran4/argus/blob/15717a9a4fd6c4fe05e6b75afabd68fa25ce4bd4/src/_old_attribute_value_parser.py\n",
    "    dirty   = re.sub(r'\\[\\[(.*?)\\|(.*?)\\]\\]', r'[[\\1<aka>\\2]]', dirty)\n",
    "    dirty   = re.sub(r' <small>(.*)</small>', r'', dirty).replace('flat list', 'flatlist').replace('Flat list', 'flatlist')\n",
    "    dirty   = re.sub(r'<br(.*)>', r'|', dirty).replace(' |small|(exec.)|', '')\n",
    "    if dirty[:7]=='{{hlist':\n",
    "        reg_list = re.compile(r'^(?i)\\{\\{(?:.*?)(?=\\|)|(?:\\|*)(?:[ ]*)class(?:[ ]*)=*(?:.*?)(?=\\|)|(?:\\|*)(?:[ ]*)list_style(?:[ ]*)=*(?:.*?)(?=\\|)|(?:\\|*)(?:[ ]*)style(?:[ ]*)=(?:.*?)(?=\\|)|(?:\\|*)(?:[ ]*)indent(?:[ ]*)=(?:.*?)(?:\\}\\}|\\|)|(?:\\|*)(?:[ ]*)item(?:\\d*)_style(?:[ ]*)=(?:.*?)(?=\\|)|(?:\\||\\*|\\#)*(?:[ ]*)([^\\|\\*\\#]+?)(?:[ ]*)(?=(?:\\||\\#|\\*))|(?:\\||\\*|\\#)*(?:[ ]*)([^\\|\\*\\#]+?)(?:[ ]*)\\}\\}$')\n",
    "        clean = [x.replace('\\n', '') for x in list(chain.from_iterable(reg_list.findall(dirty))) if x is not None and x.replace('\\n', '')!='']\n",
    "    elif dirty[:10].replace(' ', '').lower()=='{{flatlist':\n",
    "        reg_list = re.compile(r'^(?i)(?:\\{\\{(?:.*?))(?=\\|)|\\|(?:[ ]*)class(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\|*)(?:[ ]*)list_style(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\|*)(?:[ ]*)style(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\|*)(?:[ ]*)indent(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\|*)(?:[ ]*)item(?:\\d*)_style(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\*|\\#)(?:[ ]*)([^\\*\\#]+)(?:[ ]*)(?=(?:\\||\\*|\\#))|(?:\\||\\*|\\#)(?:[ ]*)([^\\*\\#\\}]+?)(?:[ ]*)\\}\\}$')\n",
    "        clean = [x.replace('\\n', '') for x in list(chain.from_iterable(reg_list.findall(dirty))) if x is not None and x.replace('\\n', '')!='']\n",
    "    elif dirty[:11].replace(' ', '').lower()=='{{plainlist':\n",
    "        reg_list = re.compile(r'^(?i)(?:\\{\\{(?:.*?))(?=\\|)|\\|(?:[ ]*)class(?:[ ]*)=(?:.*?)(?=\\||\\})|\\|(?:[ ]*)list_style(?:[ ]*)=(?:.*?)(?=\\||\\})|\\|(?:[ ]*)style(?:[ ]*)=(?:.*?)(?=\\||\\})|\\|(?:[ ]*)indent(?:[ ]*)=(?:.*?)(?=\\||\\})|\\|(?:[ ]*)item(?:\\d*)_style(?:[ ]*)=(?:.*?)(?=\\||\\})|(?:\\*|\\#)(?:[ ]*)([^\\*\\#]+)(?:[ ]*)(?=(?:\\||\\*|\\#))|(?:\\||\\*|\\#)(?:[ ]*)([^\\*\\#\\}]*?)(?:[ ]*)\\}\\}$')\n",
    "        clean = [x.replace('\\n', '') for x in list(chain.from_iterable(reg_list.findall(dirty))) if x is not None and x.replace('\\n', '')!='']\n",
    "    else:\n",
    "        clean = dirty.split(',')\n",
    "    clean = [x.replace('[[', '').replace(']]', '').strip().replace('<ext>', '<aka>').replace(' and ', '<aka>').replace(' & ', '<aka>').replace('; ', '<aka>').replace('|', '<aka>').replace('\\n*', '<aka>').split('<aka>')[0] for x in clean]\n",
    "    clean = [re.sub(r'\\([^)]*\\)', '', x) for x in clean if x not in ['{{small', 'small', '', 'ref', '</ref>', 'Music', '{{cite web', '(also Executive producer', '(exec.)', 'Jr.', '(also exec.)', '(add.)', '(co.)', '(also co-exec.)', 'exec.)', '(Executive producer', 'name=\"digitalbooklet\"', 'title=Digital Booklet - DAMN. copy.pdf', 'url=https://www.docdroid.net/rYHzAok/digital-booklet-damn-copy.pdf.html', 'abbr', '=', ')', '{{cite news', '{{small|}}', 'Small', 'b', 'Sakiya Sandifer', 'a', '[a]', 'exec.', 'executive producer', 'name=\"liner\"', 'apl.de.ap', '2012', '2012}}</inner><close>&lt;/ref&gt;</close></ext>', 'name=\"allmusic\"', 'df= }}', '2015', 'deadurl=yes', 'name=\"Underthegun\"', '2013}}</inner><close>&lt;/ref&gt;</close></ext>', 'see Album credits --&gt;</comment>', 'http://repertoire.bmi.com/title.asp?blnWriter=True&blnPublisher=True&blnArtist=True&keyid=20832724&ShowNbr=0&ShowSeqNbr=0&querytype=WorkID', 'https://twitter.com/beatsbynav/status/626357968985018368', 'https://twitter.com/beatsbynav/status/626409387050049536', 'co-prod.', '[b]', 'work=Taste of Country', 'name=\"tasteofcountry\"', 'accessdate=April 19, 2010', 'page=2', 'language=en}}', 'https://twitter.com/ovo40/status/626319695784857600', 'publisher=Broadcast Music Incorporated', '(', '(12\"', '(also', '(also', '(also executive producer', '(also executive producer', '(co-producer', '2015}}</inner><close>&lt;/ref&gt;</close></ext>', '{{collapsible list', 'work=Hamada Mania Music Blog', 'url=http://hamadamania.com/2015/04/20/adam-lambert-unveils-new-album-the-original-high-artwork/', 'title=Ghost Town']]\n",
    "       \n",
    "    clean = [x.strip().replace(' &amp', '') for x in clean if x.strip()!='']\n",
    "    final = []\n",
    "    for x in clean:\n",
    "        if x[:2]=='* ':\n",
    "            final.append(x[2:])\n",
    "        elif x[:1]=='*':\n",
    "            final.append(x[1:])\n",
    "        else:\n",
    "            final.append(x)\n",
    "    final = list(set(final))\n",
    "    return(final)\n",
    "\n",
    "#print('\\nTEST HLIST')\n",
    "#y = all_tracks[all_tracks.title=='good drank'].wiki[5]\n",
    "#y = y['writer']\n",
    "#print(y, '\\n')\n",
    "#print(parse_lists(y))\n",
    "#\n",
    "#print('\\nTEST FLATLIST')\n",
    "#y = all_tracks[all_tracks.title==\"everything i didn't say\"].wiki[35]\n",
    "#y = y['producer']\n",
    "#print(y, '\\n')\n",
    "#print(parse_lists(y))\n",
    "#\n",
    "#print('\\nTEST NO LIST')\n",
    "#y = all_tracks[all_tracks.title=='i can do anything'].wiki[28]\n",
    "#y = y['producer']\n",
    "#print(y, '\\n')\n",
    "#print(parse_lists(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tracks = all_tracks[(all_tracks.title!='') & \\\n",
    "                        (all_tracks.art_1!='') & \\\n",
    "                        (all_tracks.cred_found==1) & \\\n",
    "                        (all_tracks.year > 2006) & \\\n",
    "                        (all_tracks['rank'] <=50)]\n",
    "parsed = {}\n",
    "for index, track in all_tracks.iterrows():\n",
    "    wiki = track.wiki\n",
    "    try:\n",
    "        composer = parse_list(wiki['composer'])\n",
    "    except:\n",
    "        composer = []\n",
    "    try:\n",
    "        producer = parse_list(wiki['producer'])\n",
    "    except:\n",
    "        producer = []\n",
    "    try:\n",
    "        writer = parse_list(wiki['writer'])\n",
    "    except:\n",
    "        writer = []\n",
    "    parsed[index] = {'composer': composer,\n",
    "                     'producer': producer,\n",
    "                     'writer':   writer}\n",
    "  \n",
    "token = util.prompt_for_user_token('hoyablues', \n",
    "                           scope='playlist-modify-public', \n",
    "                           client_id='', \n",
    "                           client_secret='') \n",
    "spt = spotipy.Spotify(auth=token)\n",
    "\n",
    "# get uri for each track\n",
    "def spotify_search(row):\n",
    "    results = spt.search(q=row['art_1']+' '+row['title'], type='track', limit=1)\n",
    "    try:\n",
    "        return \"spotify:track:\" + results['tracks']['items'][0]['id']\n",
    "    except:\n",
    "        return None\n",
    "all_tracks['uri'] = all_tracks.apply(spotify_search, axis = 1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# decided to filter out any credit where the person has at least 3 credits on the billboard\n",
    "# need to consolidate and clean up the names and remove any credit for someone listed on the track\n",
    "# the reason for this is that we already \"know\" those people but want to know the hidden players\n",
    "def alias_switcher(alias, crosswalk=var_lu):\n",
    "    try:\n",
    "        return(crosswalk[alias])\n",
    "    except:\n",
    "        return(alias)\n",
    "\n",
    "all_artists = pd.read_pickle('../02_data/charts.p')\n",
    "all_artists_lu = {}\n",
    "for index, row in all_artists.iterrows():\n",
    "    art = [x.strip() for x in row.all_art.split('|')]\n",
    "    art +=[alias_switcher(x) for x in art]\n",
    "    art = [x for x in art if x is not None]\n",
    "    all_artists_lu[row.art_1.lower()+'-'+row.title.lower()] = list(set(art))\n",
    "    \n",
    "clean_parse = {}\n",
    "for k, v in parsed.items():\n",
    "    ghosts = list(set(v['writer']+v['composer']+v['producer']))\n",
    "    ghosts = list(set([alias_switcher(g) for g in ghosts]))\n",
    "    # filter out any \"ghost\" who was listed on the track\n",
    "    listed_artists = all_artists_lu[all_tracks.ix[k].art_1+'-'+all_tracks.ix[k].title]\n",
    "    clean_parse[k] = [g for g in ghosts if g not in listed_artists] # filters out the artist\n",
    "    clean_parse[k] = [g for g in ghosts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at top writers\n",
    "all_names = []\n",
    "for k, v in clean_parse.items():\n",
    "    all_names+=v\n",
    "    \n",
    "counts = sorted(Counter(all_names).items(), key=lambda pair: pair[1], reverse=True)\n",
    "counts[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets choose a cutoff threshold:\n",
    "thresh = 3\n",
    "ggplot(pd.DataFrame([x[1] for x in counts if x[1] >= thresh], columns=['n_creds']), \n",
    "       aes(x='n_creds')) + geom_histogram(bins=round(len(counts)**(1/2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final list of credits\n",
    "final_creds = [x[0] for x in counts if x[1] >= thresh and x[0] is not None]\n",
    "\n",
    "# create credits lookup\n",
    "for k, values in clean_parse.items():\n",
    "    clean_parse[k] = [re.sub(r'\\([^)]*\\)', '', v) for v in values if v in final_creds]\n",
    "\n",
    "# create artist:id lookup\n",
    "final_creds = []\n",
    "for v in clean_parse.values():\n",
    "    final_creds.extend(v)\n",
    "final_creds = list(set(final_creds))\n",
    "\n",
    "# create lookups for nodes\n",
    "ghost2id = {}; id2ghost = {}\n",
    "for k, v in enumerate(final_creds):\n",
    "    id2ghost[k]=v\n",
    "    ghost2id[v]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pivot credits long for node statistics\n",
    "edges = []; cred_long = []\n",
    "for k, v in clean_parse.items():\n",
    "    if len(v)>0:\n",
    "        track = all_tracks.loc[k]\n",
    "        for cred in v:\n",
    "            cred_long.append((cred, track['art_1'], track['title'], track['year'], track['rank'], track['weeks'], track['uri']))\n",
    "            \n",
    "        # create distinct edge combinations\n",
    "        for combo in combinations(v, r=2):\n",
    "            edges.append((combo[0], combo[1], track['art_1'], track['title'], track['year'], track['rank'], track['weeks'], track['uri']))\n",
    "\n",
    "nodes = pd.DataFrame.from_records(cred_long, columns=['ghost', 'artist', 'title', 'year', 'rank', 'weeks', 'uri'])\n",
    "edges = pd.DataFrame.from_records(edges, \n",
    "                                  columns=['ghost_1', 'ghost_2', 'artist', 'title', 'year', 'rank', 'weeks', 'uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get top 3 cowriters for each ghost and store as html ordered list\n",
    "cowriters = pd.concat([edges[['ghost_1', 'ghost_2']], \n",
    "                       edges[['ghost_2', 'ghost_1']].rename(index=str, columns={\"ghost_1\": \"ghost_2\", \"ghost_2\": \"ghost_1\"})], axis=0, ignore_index=True)\\\n",
    "              .groupby(by=['ghost_1', 'ghost_2']).size().rename('counts').reset_index()\n",
    "    \n",
    "cowriters['rank'] = cowriters.sort_values(by=['ghost_1','ghost_2', 'counts'])\\\n",
    "                             .groupby('ghost_1')['counts'].rank(method='first', na_option='top')    \n",
    "cowriters = cowriters.query('rank < 4')\n",
    "cowriters['details'] = cowriters.ghost_2 + ' (' + cowriters['counts'].apply(str) + ')'\n",
    "cowriters = cowriters.groupby(by='ghost_1').agg({'details': ['unique']}).reset_index()\n",
    "cowriters.columns = ['_'.join(x).rstrip('_') for x in cowriters.columns.ravel()]\n",
    "for i, row in cowriters.iterrows():\n",
    "    cowriters.ix[i, 'details_unique'] = '<ol>{}</ol>'.format(''.join(['<li>'+d+'</li>' for d in row['details_unique']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create edge and node stats\n",
    "def summarize_node(row):\n",
    "    sent = '{} had {} tracks in {}, peaking at {} and surviving {} total weeks on the charts.'\n",
    "    if row.year_min==row.year_max:\n",
    "        yr = row.year_min\n",
    "    else:\n",
    "        yr = str(row.year_min)+'-'+str(row.year_max)\n",
    "    return(sent.format(row.ghost, row.year_count, yr, row.rank_min, row.weeks_sum))\n",
    "\n",
    "nodes = nodes.groupby(by='ghost').agg({'weeks':['max', 'mean', 'sum'], \n",
    "                                       'rank': ['min', 'mean'],\n",
    "                                       'year': ['min', 'max', 'count'], \n",
    "                                       'uri': ['unique']}).reset_index()\n",
    "nodes.columns = ['_'.join(x).rstrip('_') for x in nodes.columns.ravel()]\n",
    "nodes['description'] = nodes.apply(lambda row: summarize_node(row), axis=1)\n",
    "nodes = pd.merge(nodes, cowriters, how='left', left_on='ghost', right_on='ghost_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a playlist for each ghost\n",
    "# Delete my current \"ghosts\" playlists\n",
    "for plist in spt.user_playlists('hoyablues')['items']:\n",
    "   if plist['name'][:9]=='Ghost :: ':\n",
    "       spt.user_playlist_unfollow('hoyablues', plist['id'])       \n",
    "        \n",
    "nodes['pid'] = None\n",
    "for i, row in nodes.iterrows():\n",
    "    tracks = [t for t in row['uri_unique'] if t is not None and len(t)==36]\n",
    "    playlist = spt.user_playlist_create('hoyablues', 'Ghost :: '+row['ghost'])\n",
    "    spt.user_playlist_add_tracks('hoyablues', playlist_id=playlist['id'], tracks=tracks)\n",
    "    nodes.ix[i, 'pid'] = playlist['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group up the edge connections\n",
    "def summarize_edge(row):\n",
    "    sent = '{} & {} worked on {} {} together, peaking at {}.'\n",
    "    if row.year_count > 1:\n",
    "        pl = 'songs'\n",
    "    else:\n",
    "        pl = 'song'\n",
    "    return(sent.format(row.ghost_1, row.ghost_2, row.year_count, pl, row.rank_min))\n",
    "\n",
    "edges = edges.groupby(by=['ghost_1', 'ghost_2']).agg({'weeks': ['max', 'mean', 'sum'], \n",
    "                                              'rank':  ['min', 'mean'],\n",
    "                                              'year':  ['min', 'max', 'count']}).reset_index()\n",
    "edges.columns = ['_'.join(x).rstrip('_') for x in edges.columns.ravel()]\n",
    "edges['description'] = edges.apply(lambda row: summarize_edge(row), axis=1)\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT GRAPH FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "for index, e in edges.iterrows():\n",
    "    G.add_edge(ghost2id[e.ghost_1], ghost2id[e.ghost_2], weight=e.year_count, label=e.description)\n",
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for index, n in nodes.iterrows():\n",
    "    try:\n",
    "        cowriters = n.details_unique.replace('\"', '').replace(\"'\", \"\")\n",
    "    except:\n",
    "        cowriters = ''\n",
    "    G.add_node(ghost2id[n.ghost], size=n.year_count, weight=n.year_count, \n",
    "               info  = cowriters,\n",
    "               name  = n.ghost,\n",
    "               label = n.description, playlist=n.pid)\n",
    "\n",
    "pos = nx.spring_layout(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_graphml(G, \"g.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('../02_data/graph.json', 'w') as outfile:\n",
    "#     # beginning\n",
    "#     outfile.write('{\\n  \"graph\": [],\\n  \"links\": [\\n')\n",
    "#     # end section\n",
    "#     for i, e in edges.iterrows():\n",
    "#         if i==max(edges.index):\n",
    "#             end = '}],'\n",
    "#         else:\n",
    "#             end = '},'\n",
    "#         outfile.write('    {}\"source\": {}, \"target\": {}{}\\n'.format('{', ghost2id[e.ghost_1], ghost2id[e.ghost_2], end))\n",
    "#     # node section\n",
    "#     outfile.write('  \"nodes\": [\\n')\n",
    "#     for i, n in nodes.iterrows():\n",
    "#         if i==max(nodes.index):\n",
    "#             end = '}],'\n",
    "#         else:\n",
    "#             end = '},'\n",
    "#         outfile.write('    {}\"size\": {}, \"score\": {}, \"node_id\": {}, \"id\": \"{}\", \"type\": \"circle\"{}\\n'\n",
    "#                       .format('{', (n.year_count*2)**500, round(n.rank_min), ghost2id[n.ghost], n.ghost.replace('\"', '').replace(\"'\", ''), end))\n",
    "#     outfile.write('  \"directed\": false,\\n  \"multigraph\": false\\n}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
