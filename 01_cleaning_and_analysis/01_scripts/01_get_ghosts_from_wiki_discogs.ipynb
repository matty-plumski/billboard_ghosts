{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Producers/Writers for Billboard Hot 100 Songs since 2000\n",
    "DB Fowler\n",
    "\n",
    "Note: this script is not an exhaustive search for every track and includes moderate data cleaning.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "from requests import get\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "from billboard import ChartData\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wptools\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import *\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Billboard Functions\n",
    "* Scrape Billboard charts and store songs' metadata (artist, title etc.)\n",
    "* Store in Pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def artist_cleaner(art_dirty):\n",
    "    '''\n",
    "        Takes an artist string and splits into a list of individual artists.\n",
    "    '''\n",
    "    return(re.sub('(\\\\s+([fF]eat\\\\.|[fF]eat[uring]{5}|[Cc]o-[sS]tarring|&|[Xx]|\\\\+|/|[oO][rR]|,|[aA]nd|AND|(?:[dD]uet\\\\s*)?[wW]ith|WITH|[Vv][Ss])\\\\s+|,|/|\\\\()', '|', art_dirty))           \n",
    "\n",
    "def parse_chart(chart):\n",
    "    '''\n",
    "        Takes a Billboard chart object and extracts relevant details.\n",
    "        Returns list of Billboard songs.\n",
    "    '''\n",
    "    details = []\n",
    "    for entry in chart:\n",
    "        clean_art = artist_cleaner(entry.artist)\n",
    "        details+=[[chart.date,  entry.artist, entry.title, entry.rank, \\\n",
    "                  entry.weeks, entry.spotifyID, entry.spotifyLink, \n",
    "                  entry.videoLink, clean_art, clean_art.split('|')[0], \\\n",
    "                  int(chart.date[:4])]]\n",
    "    return(details)\n",
    "\n",
    "def process_charts(min_year=2016):\n",
    "    '''\n",
    "        For every chart until min_year, downloads and parses chart.\n",
    "        Returns pandas dataframe of all charts formatted.\n",
    "    '''\n",
    "    all_charts = []\n",
    "    chart = ChartData('hot-100')\n",
    "    while chart.previousDate[:4]!=str(min_year):\n",
    "        all_charts.extend(parse_chart(chart))\n",
    "        chart = ChartData('hot-100', chart.previousDate)\n",
    "    all_charts = pd.DataFrame(all_charts, columns=['date',  'artist', 'title', 'rank', 'weeks', \\\n",
    "                                                   'spotifyID', 'spotifyLink', 'videoLink', \\\n",
    "                                                   'all_art', 'art_1', 'year'])\n",
    "    return(all_charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Functions\n",
    "* Search for a song on Wikipedia and download it's infobox.\n",
    "* Add a flag for if the Infobox contained songwriter/producer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Billboard Charts...\n",
      "Found 416 Billboard Tracks...\n"
     ]
    }
   ],
   "source": [
    "print('Downloading Billboard Charts...')\n",
    "charts = process_charts(min_year=2016)\n",
    "rolled = charts.groupby(['art_1', 'title'], as_index=False).agg({'year':'min', 'rank': 'min', 'weeks': 'max'})\n",
    "rolled.art_1 = rolled.art_1.map(lambda x: x.lower())\n",
    "rolled.title = rolled.title.map(lambda x: x.lower())\n",
    "print('Found {} Billboard Tracks...'.format(len(rolled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wiki_pg(row):\n",
    "    '''\n",
    "        Given a song's title and artist, search for it's Wiki page.\n",
    "    '''\n",
    "    query = str(row['title']) + ' (' + str(row['art_1']) + ' song)'\n",
    "    try:\n",
    "        results = wikipedia.page(query)\n",
    "    except PageError as pg_error:\n",
    "        try:\n",
    "            results = wikipedia.page(pg_error.pageid)\n",
    "        except:\n",
    "            results = None\n",
    "    except DisambiguationError as dis_error:\n",
    "        results = None\n",
    "        #choice = process.extractOne(query, dis_error.options, scorer=fuzz.token_sort_ratio)\n",
    "        #try:\n",
    "        #    results = wikipedia.page(choice[0].replace('\"', ''))\n",
    "        #except:\n",
    "        #    results = None\n",
    "    return(results)\n",
    "\n",
    "def wiki_credits(df):\n",
    "    '''\n",
    "        Find the Wiki page for a song and download the InfoBox into the DF.\n",
    "    '''\n",
    "    wiki_results = []\n",
    "    for index, row in df.iterrows():\n",
    "        info = None\n",
    "        results = wiki_pg(row)\n",
    "        if results:\n",
    "            url_end = results.url.replace('https://en.wikipedia.org/wiki/', '')\n",
    "            try:\n",
    "                info = wptools.page(url_end, silent=True).get_parse().infobox\n",
    "                info = dict((k.lower(), v) for k, v in dict(info).items())\n",
    "            except:\n",
    "                info = None\n",
    "        else:\n",
    "            info = None\n",
    "        wiki_results.append(info)\n",
    "    df['wiki'] = wiki_results\n",
    "    \n",
    "    # add wiki creds found flag\n",
    "    wiki_cred = []\n",
    "    for i in df.wiki:\n",
    "        if any(word in str(i).lower() for word in ['producer', 'writer']):\n",
    "            wiki_cred.append(1)\n",
    "        elif i is None:\n",
    "            wiki_cred.append(0)\n",
    "        else:\n",
    "            wiki_cred.append(0)\n",
    "    df['cred_found'] = wiki_cred\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCOGS Functions\n",
    "* Search Discogs DB for a track's credits\n",
    "* Find a track's master, then all associated releases, then the distinct list of credits for those releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class discogs(object):\n",
    "    '''\n",
    "        Used to monitor request timeout across multiple queries\n",
    "    '''\n",
    "    def __init__(self, token=''):\n",
    "        self.url     = 'https://api.discogs.com/database/search'\n",
    "        self.headers = {'accept-encoding': 'gzip, deflate', 'user-agent': 'cde456'} \n",
    "        self.token   = token\n",
    "        self.rate_remain = 60\n",
    "\n",
    "    def search_masters(self, artist, title, verbose=False):\n",
    "        '''\n",
    "            Query all masters matching artist and title; \n",
    "            Returns list of master urls\n",
    "        '''\n",
    "        if verbose==True: print(artist, title)\n",
    "        rate_remain = self.rate_remain\n",
    "        params      = {'token': self.token, 'type' : 'master', 'artist': artist, 'release_title': title}\n",
    "        masters     = []\n",
    "        if rate_remain==0:\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            try:\n",
    "                response = requests.get(self.url, headers=self.headers, params=params, verify=False)\n",
    "                self.rate_remain = int(response.headers['X-Discogs-Ratelimit-Remaining'])\n",
    "                response = response.json()['results']\n",
    "                if len(response)>0:\n",
    "                    for master in response:\n",
    "                        masters.append(master['resource_url'])\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print('Error: {}, Remaining: {}'.format(e, self.rate_remain))\n",
    "                time.sleep(5)\n",
    "            if verbose==True: print('Remaining: {}'.format(self.rate_remain))\n",
    "            return(masters)\n",
    "\n",
    "    def get_main_releases(self, masters=[], verbose=False):\n",
    "        '''\n",
    "            Finds all releases associated with a master.\n",
    "        '''\n",
    "        if len(masters) > 0:\n",
    "            rate_remain      = self.rate_remain\n",
    "            params           = {'token': self.token}\n",
    "            main_releases    = []\n",
    "            for master in masters:\n",
    "                if rate_remain==0:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    try:\n",
    "                        response = requests.get(master, headers=self.headers, params=params, verify=False)\n",
    "                        self.rate_remain = int(response.headers['X-Discogs-Ratelimit-Remaining'])\n",
    "                        time.sleep(1)\n",
    "                        main_releases.append((response.json()['main_release_url']))\n",
    "                    except Exception as e:\n",
    "                        print('Error: {}, Remaining: {}'.format(e, self.rate_remain))\n",
    "                        time.sleep(5)\n",
    "            return(list(set(main_releases)))\n",
    "        else:\n",
    "            return([])\n",
    "\n",
    "    def get_credits(self, releases, verbose=False):\n",
    "        '''\n",
    "            Returns all credits related to a release\n",
    "        '''\n",
    "        if len(releases)>0:\n",
    "            rate_remain      = self.rate_remain\n",
    "            params           = {'token': self.token}\n",
    "            credits          = []\n",
    "            for release in releases:\n",
    "                if rate_remain==0:\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    try:\n",
    "                        response = requests.get(release, headers=self.headers, params=params, verify=False)\n",
    "                        self.rate_remain = int(response.headers['X-Discogs-Ratelimit-Remaining'])\n",
    "                        time.sleep(1)\n",
    "                        credits.extend([(extra['id'], extra['role']) for extra in response.json()['extraartists']])\n",
    "                    except Exception as e:\n",
    "                        print('Error: {}, Remaining: {}'.format(e, self.rate_remain))\n",
    "                        time.sleep(5)\n",
    "            return(pd.Series({'credits': set(credits)}))\n",
    "        else:\n",
    "            return(pd.Series({'credits': np.nan}))\n",
    "\n",
    "    def get_artist_details(self, df):\n",
    "        '''\n",
    "            Returns all artist details for all artists in the credits lists for all songs.\n",
    "            Rather than request every detail as scraped, this way makes sure we don't make\n",
    "            duplicitive requests.\n",
    "            Control for poorly defined JSON returned from DISCOGS\n",
    "        '''\n",
    "        rate_remain   = self.rate_remain\n",
    "        params        = {'token': self.token}\n",
    "        all_ids       = []; artist_lookup = {}\n",
    "        for i in list(df[df.credits.notnull()].credits):\n",
    "            all_ids.extend([i[0] for i in list(i)])\n",
    "        all_ids = list(set(all_ids))\n",
    "        \n",
    "        for art_id in all_ids:\n",
    "            if rate_remain==0:\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                try:\n",
    "                    response = requests.get('https://api.discogs.com/artists/{}'.format(art_id), \n",
    "                                            headers=self.headers, params=params, verify=False)\n",
    "                    self.rate_remain = int(response.headers['X-Discogs-Ratelimit-Remaining'])\n",
    "                    time.sleep(1)\n",
    "                    response = response.json()\n",
    "                    try:\n",
    "                        name = response['name']\n",
    "                    except:\n",
    "                        name = []\n",
    "                    try:\n",
    "                        realname = response['realname']\n",
    "                    except:\n",
    "                        realname = []\n",
    "                    try:\n",
    "                        groups = [i['name'] for i in response['groups']]\n",
    "                    except:\n",
    "                        groups = []\n",
    "                    try:\n",
    "                        namevariations = response['namevariations']\n",
    "                    except:\n",
    "                        namevariations = []\n",
    "                    artist_lookup[art_id] = {'name':       name, \n",
    "                                             'realname':   realname, \n",
    "                                             'groups':     groups,\n",
    "                                             'variations': namevariations}\n",
    "                except Exception as e:\n",
    "                    print('Error: {}, Remaining: {}'.format(e, self.rate_remain))\n",
    "                    time.sleep(5)\n",
    "        if artist_lookup=={}:\n",
    "            return(np.nan)\n",
    "        else:\n",
    "            return(artist_lookup)\n",
    "\n",
    "def retrieve_credits(session, row, verbose=False):\n",
    "    '''\n",
    "        Search for a song's masters >> all releases >> all unique credits\n",
    "    '''\n",
    "    masters  = session.search_masters(artist=row['art_1'], title=row['title'], verbose=verbose)\n",
    "    releases = session.get_main_releases(masters, verbose=verbose)\n",
    "    credits  = session.get_credits(releases,      verbose=verbose)\n",
    "    return(credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Billboard Charts...\n",
      "Found 6718 Billboard Tracks...\n",
      "Searching for Wiki credits...\n",
      "6117 tracks found wiki credits...\n",
      "Searching for 601 additional credits on Discogs...\n",
      "Searching for Discogs Credits...\n",
      "Saving Files...\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Downloading Billboard Charts...')\n",
    "    charts = process_charts(min_year=1999)\n",
    "    charts.to_pickle('../02_data/charts.p')\n",
    "    rolled = charts.groupby(['art_1', 'title'], as_index=False).agg({'year':'min', 'rank': 'min', 'weeks': 'max'})\n",
    "    rolled.art_1 = rolled.art_1.map(lambda x: x.lower())\n",
    "    rolled.title = rolled.title.map(lambda x: x.lower())\n",
    "    print('Found {} Billboard Tracks...'.format(len(rolled)))\n",
    "    \n",
    "    print('Searching for Wiki credits...')\n",
    "    rolled    = wiki_credits(rolled)\n",
    "    found     = rolled[rolled.cred_found==1].copy()\n",
    "    not_found = rolled[rolled.cred_found==0].copy()\n",
    "    print('{} tracks found wiki credits...\\nSearching for {} additional credits on Discogs...'.format(len(found), len(not_found)))\n",
    "    \n",
    "    print('Searching for Discogs Credits...')\n",
    "    d = discogs()\n",
    "    not_found['credits'] = not_found.apply(lambda row: retrieve_credits(d, row, verbose=False), axis=1)\n",
    "    artist_lookup        = d.get_artist_details(not_found)\n",
    "    found['credits']     = np.nan\n",
    "    \n",
    "    print('Saving Files...')\n",
    "    final = pd.concat([found, not_found])\n",
    "    final.to_pickle('../02_data/all_tracks.p')\n",
    "    pickle.dump(artist_lookup, open('../02_data/artist_lookup.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
